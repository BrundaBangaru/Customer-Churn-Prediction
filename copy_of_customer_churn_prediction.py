# -*- coding: utf-8 -*-
"""Copy of Customer-Churn-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HnSEQ_1uOg3tvD8gkEQpZjP5XMWyxyx2
"""

pip install xgboost imbalanced-learn

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE


# 1. LOAD DATA
try:
    df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
    print("‚úÖ Dataset Loaded")
except FileNotFoundError:
    print("‚ùå Error: File not found. Ensure the CSV is in your VS folder.")
    exit()

# 2. DROP NON-PREDICTIVE COLUMNS
# customerID is unique to every person and has no predictive power.
df.drop("customerID", axis=1, inplace=True)

# 3. HANDLE MISSING VALUES & DATA TYPES
# TotalCharges is often read as an 'object' due to empty strings.
# We convert it to numeric and coerce errors to NaN.
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Check for nulls (The 11 empty strings in TotalCharges become Nulls)
print(f"Missing values found:\n{df.isnull().sum()}")

# Fill missing TotalCharges with the median
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())

# 4. ENCODE CATEGORICAL DATA
# We use Label Encoding for binary features and one-hot encoding logic for others.
le = LabelEncoder()

# Select columns with text data
categorical_cols = df.select_dtypes(include=['object']).columns

for col in categorical_cols:
    # If the column is the target (Churn), map it manually to 1 and 0
    if col == 'Churn':
        df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})
    else:
        # Encode all other categorical text into numbers for the ML model
        df[col] = le.fit_transform(df[col])

# 5. SPLIT DATA (Features vs Target)
X = df.drop('Churn', axis=1)
y = df['Churn']

# 80% Training, 20% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 6. FEATURE SCALING
# Machine learning models perform better when all numbers are on the same scale (e.g., 0 to 1)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Preprocessing Complete")
print(f"Training shape: {X_train_scaled.shape}")
print(f"Testing shape: {X_test_scaled.shape}")

# Save the preprocessed data to a CSV for your records
df.to_csv("preprocessed_churn_data.csv", index=False)



# Load the data from the previous preprocessing step
df = pd.read_csv("preprocessed_churn_data.csv")

# --- 1. SERVICE ENGAGEMENT (The "Sticky" Factor) ---
# Goal: Identify how many extra services a customer uses.
# Logic: Customers with more services are harder to lose (higher switching costs).
service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']
# In our preprocessed data, 'Yes' was encoded. Let's assume 'Yes' is the higher value (usually 2).
# A simpler way is to count how many services they have active.
df['ServiceCount'] = df[service_cols].gt(0).sum(axis=1)

# --- 2. FINANCIAL VELOCITY (Charge Intensity) ---
# Goal: Is the customer paying a lot relative to their time with us?
# High ChargeIntensity often marks "at-risk" new customers.
df['ChargeIntensity'] = df['MonthlyCharges'] / (df['tenure'] + 1)

# --- 3. LOYALTY SEGMENTATION (Tenure Buckets) ---
# Goal: Convert raw months into life-cycle stages.
# Machine learning models often find patterns in "groups" better than raw numbers.
df['IsNewCustomer'] = df['tenure'].apply(lambda x: 1 if x <= 12 else 0)
df['IsLoyalCustomer'] = df['tenure'].apply(lambda x: 1 if x >= 48 else 0)

# --- 4. CONTRACT TYPE RISK ---
# Goal: Highlight high-risk contracts.
# Month-to-month contracts are the biggest churn drivers.
# Assuming Month-to-month was encoded as 0 (check your label encoder)
df['IsMonthToMonth'] = df['Contract'].apply(lambda x: 1 if x == 0 else 0)

# --- 5. STREAMING BEHAVIOR ---
# Goal: See if the customer uses the telecom for entertainment.
df['IsStreamer'] = ((df['StreamingTV'] > 0) | (df['StreamingMovies'] > 0)).astype(int)

# --- 6. COST BURDEN ---
# Goal: Total charges relative to monthly charges.
df['LifetimeValueScore'] = df['TotalCharges'] / (df['MonthlyCharges'] + 1)

print("‚úÖ Feature Engineering Complete")
print(df[['ServiceCount', 'ChargeIntensity', 'IsNewCustomer', 'LifetimeValueScore']].head())

# Save for the modeling step
df.to_csv("featured_churn_data.csv", index=False)

# 1. Load data
df = pd.read_csv("featured_churn_data.csv")
X = df.drop('Churn', axis=1)
y = df['Churn']

# 2. Address Imbalance with SMOTE
# This creates synthetic examples of churners so the model learns better
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)

# 3. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_res, y_res, test_size=0.2, random_state=42
)

# 4. XGBoost - A more powerful "Boosting" model
model = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='logloss',
    use_label_encoder=False
)

model.fit(X_train, y_train)

# 5. Evaluate
y_pred = model.predict(X_test)
print(f"üéØ New Model Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
# 1. Generate Predictions
y_pred = model.predict(X_test)
y_probs = model.predict_proba(X_test)[:, 1] # Probability of churn

# 2. Classification Report (Precision, Recall, F1)
print("--- Detailed Classification Report ---")
print(classification_report(y_test, y_pred))

#7. VISUALIZING FEATURE IMPORTANCE
plt.figure(figsize=(10, 6))
importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=True)
importances.tail(10).plot(kind='barh', color='teal')
plt.title("Top 10 Features Contributing to Churn")
plt.xlabel("Importance Score")
plt.show()

# 8. CONFUSION MATRIX (For the Project Report)
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix: Churn vs Retained')
plt.show()

# 4. ROC Curve (Receiver Operating Characteristic)
# This shows the trade-off between sensitivity and specificity
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curve: Model Discriminating Power')
plt.legend(loc="lower right")
plt.show()

#Hexbin
plt.figure(figsize=(10, 7))
df_churn = df[df['Churn'] == 1]
plt.hexbin(df_churn['tenure'], df_churn['MonthlyCharges'], gridsize=25, cmap='Reds')
plt.colorbar(label='Concentration of Churning Customers')
plt.xlabel('Tenure (Months)')
plt.ylabel('Monthly Charges ($)')
plt.title('The Churn "Danger Zone": Tenure vs Monthly Charges')
plt.show()

#Density
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='ServiceCount', hue='Churn', fill=True, common_norm=False, palette='coolwarm')
plt.title('The Power of Service Entanglement (Density Plot)')
plt.xlabel('Number of Services Subscribed')
plt.ylabel('Density')
plt.show()